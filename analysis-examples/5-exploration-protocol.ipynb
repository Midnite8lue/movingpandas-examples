{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A protocol for movement data exploration\n",
    "\n",
    "This notebook presents a systematic movement data exploration protocol. \n",
    "\n",
    "Following Zuur et al.'s (2010) example, our protocol consists of a series of flexible steps that should be treated as questions to be asked of the data. The steps are grouped by problem types, based on our extension of the typology of movement data quality problems by Andrienko et al. (2016).\n",
    "\n",
    "The protocol starts with steps exploring elementary records, before it advances to steps looking into intermediate segments of consecutive records, and finally, to the steps requiring whole trajectories. This approach reflects the typical flow of movement data processing since raw data is usually provided as elementary records that need to be processes to connect consecutive records into continuous tracks that can then be split to extract individual trajectories. Consequently, choices made along the processing chain, for example, regarding how tracks are split into trajectories, will affect results of later steps (Andrienko et al. 2013, p371).  \n",
    "\n",
    "At each step of the protocol, we first describe the data problem and its potential causes, then explain the potential consequences if the problem is not identified, and finally, propose suitable exploratory analysis methods.\n",
    "\n",
    "* **A Missing data**\n",
    " * A-1 Spatial gaps & outliers\n",
    " * A-2 Temporal gaps & outliers\n",
    " * A-3 Spatiotemporal gaps\n",
    " * A-4 Attribute gaps\n",
    " * A-5 Gaps in trajectories\n",
    "* **B Precision problems**\n",
    " * B-1 Coordinate imprecision\n",
    " * B-2 Timestamp imprecision\n",
    "* **C Consistency problems**\n",
    " * C-1 Sampling heterogeneity\n",
    " * C-2 Mover heterogeneity\n",
    " * C-3 Tracker heterogeneity\n",
    "* **D Accuracy problems**\n",
    " * D-1 Object identity issues\n",
    " * D-2 Spatial inaccuracy \n",
    " * D-3 Temporal inaccuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIGSIZE = (600,400)\n",
    "SMSIZE = 300\n",
    "COLOR = 'darkblue'\n",
    "COLOR_HIGHLIGHT = 'red'\n",
    "COLOR_BASE = 'grey'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sin, cos, atan2, radians, degrees, sqrt, pi\n",
    "from datetime import datetime, date\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import movingpandas as mpd\n",
    "import datashader as ds\n",
    "import holoviews as hv\n",
    "from shapely.geometry import Point, LineString\n",
    "from holoviews.operation.datashader import datashade, spread\n",
    "from holoviews.element import tiles\n",
    "from holoviews import opts, dim \n",
    "import hvplot\n",
    "from shapely.geometry import Point\n",
    "\n",
    "R_EARTH = 6371000  # radius of earth in meters\n",
    "C_EARTH = 2 * R_EARTH * pi  # circumference\n",
    "BG_TILES = tiles.CartoLight()\n",
    "\n",
    "pd.set_option('use_inf_as_na', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_single_mover(df, mover_id, the_date):\n",
    "    tmp = df[(df.id==mover_id) & (df.index.date==the_date)]\n",
    "    gdf = gpd.GeoDataFrame(tmp.drop(['x', 'y'], axis=1), crs={'init': 'epsg:3857'}, geometry=[Point(xy) for xy in zip(tmp.x, tmp.y)])\n",
    "    plot = mp.Trajectory(gdf, 1).hvplot(title=f'Mover {mover_id} ({the_date})', c='speed_m/s', cmap='RdYlBu',  colorbar=True, clim=(0,15), \n",
    "                                        line_width=5, width=FIGSIZE[0], height=FIGSIZE[1], tiles='CartoLight')\n",
    "    return plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files = [\n",
    "    'E:/Geodata/AISDK/raw_ais/aisdk_20170701.csv',\n",
    "    #'E:/Geodata/AISDK/raw_ais/aisdk_20170702.csv',\n",
    "    #'E:/Geodata/AISDK/raw_ais/aisdk_20170703.csv',\n",
    "    #'E:/Geodata/AISDK/raw_ais/aisdk_20170704.csv',\n",
    "    #'E:/Geodata/AISDK/raw_ais/aisdk_20170705.csv',\n",
    "    #'E:/Geodata/AISDK/raw_ais/aisdk_20170706.csv',\n",
    "    'E:/Geodata/AISDK/raw_ais/aisdk_20180101.csv',\n",
    "    #'E:/Geodata/AISDK/raw_ais/aisdk_20180102.csv',\n",
    "    #'E:/Geodata/AISDK/raw_ais/aisdk_20180103.csv',\n",
    "    #'E:/Geodata/AISDK/raw_ais/aisdk_20180104.csv',\n",
    "    #'E:/Geodata/AISDK/raw_ais/aisdk_20180105.csv',\n",
    "    #'E:/Geodata/AISDK/raw_ais/aisdk_20180106.csv'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(input_files[0], nrows=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SOG'].hist(bins=100, figsize=(15,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = None\n",
    "for input_file in input_files[:2]: \n",
    "    a = pd.read_csv(input_file, usecols=['# Timestamp', 'MMSI', 'Latitude', 'Longitude', 'SOG', 'Type of mobile', 'Ship type', 'Navigational status'])\n",
    "    a = a[(a['Type of mobile'] == 'Class A') & (a.SOG>0)]\n",
    "    a.drop(columns=['Type of mobile', 'SOG'], inplace=True)\n",
    "    if df is None:\n",
    "        df = a\n",
    "    else:\n",
    "        df = df.append(a)\n",
    "    \n",
    "df.rename(columns={'# Timestamp':'time', 'MMSI':'id', 'Latitude':'lat', 'Longitude':'lon', 'Ship type':'shiptype', 'Navigational status':'navstat'}, inplace=True)\n",
    "df['time'] = pd.to_datetime(df['time'], format='%d/%m/%Y %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, 'x'], df.loc[:, 'y'] = ds.utils.lnglat_to_meters(df.lon, df.lat)\n",
    "\n",
    "df.set_index('time', inplace=True)\n",
    "\n",
    "df['navstat'] = df['navstat'].astype('category')\n",
    "df['shiptype'] = df['shiptype'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of records: {} million'.format(round(len(df)/1000000)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A) Missing data\n",
    "\n",
    "Checking for missing data is a common starting point for exploring new movement datasets. At this early stage, we usually start with raw location records that have yet to be aggregated into trajectories. Therefore, initial analyses look at elementary position records.\n",
    "\n",
    "The following protocol steps target issues of missing data with respect to movement data's spatial, temporal, and attribute dimensions.\n",
    "\n",
    "\n",
    "### A-1) Spatial gaps & outliers\n",
    "\n",
    "To gain an overview, the analysis should start from the whole time span before drilling down. Spatial context (usually in the form of base maps) is essential when assessing spatial extent and gaps because context influences movement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spatial spread / extent & outliers (whole territory / all movers / whole time span)\n",
    "\n",
    "This step addresses the question if the dataset covers the expected spatial extent. This can be as simple as checking the minimum and maximum coordinate values of the raw records. However, it is not uncommon to encounter spurious location records or outliers that are not representative of the actual covered extent. These outliers may be truly erroneous positions but can also be correct positions that happen to be located outside the usual extent. Looking at elementary position records only, it is usually not possible to distinguish these two cases. It is therefore necessary to take note of these outliers and investigate further in later steps.\n",
    "\n",
    "TODO: consequences \n",
    "\n",
    "Classic scatter plots (or point maps) are helpful at this step. Point density maps (often called heat maps) on their default settings tend to hide outliers and are therefore not recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Spatial extent: x_min={df.lon.min()}, x_max={df.lon.max()}, y_min={df.lat.min()}, y_max={df.lat.max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_basic_scatter(df, color='darkblue', title='', width=FIGSIZE[0], height=FIGSIZE[1], size=2):\n",
    "    opts.defaults(opts.Overlay(active_tools=['wheel_zoom']))\n",
    "    pts = df.hvplot.scatter(x='x', y='y', datashade=True, cmap=[color, color], frame_width=width, frame_height=height, title=str(title))\n",
    "    return BG_TILES * spread(pts, px=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_basic_scatter(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df.lon>-90) & (df.lon<90) & (df.lat>0) & (df.lat<80)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_df = df[(df.lon>0) & (df.lon<20) & (df.lat>52) & (df.lat<60)]\n",
    "cropped_df['navstat'] = cropped_df['navstat'].astype('category')\n",
    "cropped_df['shiptype'] = cropped_df['shiptype'].astype('category')\n",
    "plot_basic_scatter(cropped_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spatial gaps (selected areas / all movers / whole time span)\n",
    "\n",
    "This step addresses the question if there are spatial gaps in the data coverage. Depending on the type of movers, gaps in certain spatial contexts are to be expected. For example, we wouldn't expect taxi locations in lakes. Other gaps may indicate issues with the data collection process or the data export used to generate the analysis dataset. Therefore, it is essential to evaluate these gaps in their spatial context using base maps showing relevant geographic features, such as the road network for vehicle data or navigation markers for vessel data. The visualization scale influences which size of gaps can be discovered. However, there are of course practical limitations to exploring ever more detailed scales and resulting continuously growing numbers of gaps.\n",
    "\n",
    "TODO: consequences\n",
    "\n",
    "Point density maps are helpful since they make it easy to identify areas with low densities, ignoring occasional outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_point_density(df, title='', width=FIGSIZE[0], height=FIGSIZE[1]):\n",
    "    opts.defaults(opts.Overlay(active_tools=['wheel_zoom']))\n",
    "    pts = df.hvplot.scatter(x='x', y='y', title=str(title), datashade=True, frame_width=width, frame_height=height)\n",
    "    return BG_TILES * pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_point_density(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A-2) Temporal gaps & outliers\n",
    "\n",
    "#### Temporal extent & outliers (whole territory / all movers / whole time span)\n",
    "\n",
    "This step addresses the question if the dataset covers the expected temporal extent. Similar to exploring the spatial extent, the obvious step is to determine the minimum and maximum timestamps first. Since GPS tracking requires accurate clocks to function, time information on the tracker is usually reliable. However, it is not guaranteed that these timestamps make it through the whole data collection and (pre)processing chain leading up to the exploratory analysis. For example, in some cases, tracker (or sender) time is replaced by receiver or storage time. Thus clock errors on the receiving or storage devices can result in unexpected timestamps.\n",
    "\n",
    "TODO: consequences\n",
    "\n",
    "Temporal charts, particularly record counts over time, are helpful to gain a first impression of the overall temporal extent and whether it is continuous or split into multiple time frames with little or no data in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Temporal extent: {df.index.min()} to {df.index.max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_SAMPLE = '15min'\n",
    "\n",
    "df['id'].resample(TIME_SAMPLE).count()\\\n",
    "    .hvplot(title=f'Number of records per {TIME_SAMPLE}', width=FIGSIZE[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temporal gaps in linear sequence & temporal cycles (whole territory / all movers / time spans)\n",
    "\n",
    "This step addresses the question if there are temporal gaps in the dataset. Temporal gaps can be due to scheduled breaks in data collection, deliberate choices during data export, as well as unintended issues during data collection or (pre)processing. Similar to exploring spatial gaps, the temporal scale influences which size of gaps can be discovered. Temporal gaps can be one-time events or exhibit reoccurring patterns. For example, daily and weekly cycles are typical for human movement data.\n",
    "\n",
    "TODO: consequences\n",
    "\n",
    "Two-dimensional time histograms are helpful at this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_df = df['id'].groupby([df.index.hour, pd.Grouper(freq='d')]).count().to_frame(name='n')\n",
    "counts_df.rename_axis(['hour', 'day'], inplace=True)\n",
    "counts_df.hvplot.heatmap(title='Record count', x='hour', y='day', C='n', width=FIGSIZE[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A-3) Spatiotemporal changes / gaps\n",
    "\n",
    "While the previous two steps looked at spatial gaps over the whole time span or temporal gaps for the whole territory, this step aims to explore spatiotemporal changes and gaps.\n",
    "\n",
    "#### Changing extent\n",
    "\n",
    "This step addresses the question whether there are changes in spatial extent over time. Changing spatial extent may be due to planned extensions or reductions of the data collection / observation area. Similarly, the extent is also expected to shift if the movers collectively change their location, as is the case, for example, with tracks of migrating birds.\n",
    "\n",
    "TODO: consequences\n",
    "\n",
    "Small multiples are helpful since they provide a quick way to compare extents during different time spans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multiple_by_day(df, day):\n",
    "    return plot_basic_scatter(df[df.index.date==day], title=day, width=SMSIZE, height=SMSIZE)\n",
    "    \n",
    "def plot_multiples_by_day(df):\n",
    "    days = df.index.to_period('D').unique()\n",
    "    a = None\n",
    "    for a_day in days:\n",
    "        a_day = a_day.to_timestamp().date()\n",
    "        plot = plot_multiple_by_day(df, a_day)\n",
    "        if a is None: a = plot\n",
    "        else: a = a  + plot\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_multiples_by_day(df).cols(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_multiples_by_day(cropped_df).cols(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multiple_by_hour_of_day(df, hour, fun):\n",
    "    return fun(df[df.index.hour==hour], title=hour, width=SMSIZE, height=SMSIZE)\n",
    "    \n",
    "def plot_multiples_by_hour_of_day(df, hours=range(0,24), fun=plot_basic_scatter):\n",
    "    a = None\n",
    "    for hour in hours:\n",
    "        plot = plot_multiple_by_hour_of_day(df, hour, fun)\n",
    "        if a is None: a = plot\n",
    "        else: a = a + plot\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plot_multiples_by_hour_of_day(df[df.shiptype=='Fishing']).cols(2)\n",
    "plot_multiples_by_hour_of_day(df, hours=[6,7,8,9]).cols(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temporary gaps\n",
    "\n",
    "This step addresses the question whether there are temporary gaps in the overall spatial coverage. Like temporary changes in the overall extent, temporary gaps can be due to mover behavior, as well as planned and unplanned changes of the data collection or (pre)processing workflows.\n",
    "\n",
    "TODO: consequences\n",
    "\n",
    "Small multiples of density maps or animated density maps are helpful at this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_multiples_by_hour_of_day(cropped_df, hours=[6,7,8,9], fun=plot_point_density).cols(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A-4) Attribute gaps\n",
    "\n",
    "Some attributes may only be available during certain time spans / or in certain areas.\n",
    "\n",
    "#### Spatial attribute gaps\n",
    "\n",
    "This step addresses the question if there are areas with missing attribute data. Locally missing attribute data can be due to heterogeneous data collection system setups.\n",
    "\n",
    "TODO: consequences\n",
    "\n",
    "The methods used to explore spatial extent and gaps can be adopted to missing attribute data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORY = 'shiptype' #'navstat'\n",
    "\n",
    "cats = df[CATEGORY].unique()\n",
    "#[cat for cat in cats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = {} \n",
    "for cat in cats:\n",
    "    cmap[cat] = COLOR_BASE\n",
    "cmap['Unknown value'] = COLOR_HIGHLIGHT\n",
    "cmap['Undefined'] = COLOR_HIGHLIGHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_categorized_scatter(df, cat, title='', width=SMSIZE, height=SMSIZE, cmap=cmap):\n",
    "    opts.defaults(opts.Overlay(active_tools=['wheel_zoom']))\n",
    "    pts = df.hvplot.scatter(x='x', y='y', datashade=True, by=cat, colormap=cmap, legend=True, frame_width=width, frame_height=height, title=str(title))\n",
    "    return BG_TILES * pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "unknown = df[(df[CATEGORY]=='Unknown value') | (df[CATEGORY]=='Undefined')]\n",
    "known = df[(df[CATEGORY]!='Unknown value') & (df[CATEGORY]!='Undefined')]\n",
    "\n",
    "( plot_categorized_scatter(df, CATEGORY, title='Categorized', width=SMSIZE, height=SMSIZE, cmap=cmap) + \n",
    "  plot_basic_scatter(unknown, COLOR_HIGHLIGHT, title='Unknown only', width=SMSIZE, height=SMSIZE, size=1) +\n",
    "  plot_basic_scatter(known, COLOR_BASE, title='Known only', width=SMSIZE, height=SMSIZE, size=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temporal attribute gaps\n",
    "\n",
    "This step addresses the question if there are temporary gaps in attribute data. Changes to the data collection or (pre)processing workflow can affect which attributes are available during certain time spans.\n",
    "\n",
    "TODO: consequences\n",
    "\n",
    "The methods used to explore temporal extent and gaps can be adopted to missing attribute data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_multiples_by_day(unknown).cols(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA PREPARATION: Computing segment information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_difference(row):\n",
    "    t1 = row['prev_t']\n",
    "    t2 = row['t']\n",
    "    return (t2-t1).total_seconds()\n",
    "\n",
    "def speed_difference(row):\n",
    "    return row['speed_m/s'] - row['prev_speed']\n",
    "\n",
    "def acceleration(row):\n",
    "    if row['diff_t_s'] == 0:\n",
    "        return None\n",
    "    return row['diff_speed'] / row['diff_t_s']\n",
    "\n",
    "def spherical_distance(lon1, lat1, lon2, lat2):\n",
    "    delta_lat = radians(lat2 - lat1)\n",
    "    delta_lon = radians(lon2 - lon1)\n",
    "    a = sin(delta_lat/2) * sin(delta_lat/2) + cos(radians(lat1)) * cos(radians(lat2)) * sin(delta_lon/2) * sin(delta_lon/2)\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    dist = R_EARTH * c\n",
    "    return dist\n",
    "\n",
    "def distance_to_prev(row):\n",
    "    return spherical_distance(row['prev_lon'], row['prev_lat'], row['lon'], row['lat'])\n",
    "    \n",
    "def distance_to_next(row):\n",
    "    return spherical_distance(row['next_lon'], row['next_lat'], row['lon'], row['lat'])\n",
    "\n",
    "def direction(row):\n",
    "    lon1, lat1, lon2, lat2 = row['prev_lon'], row['prev_lat'], row['lon'], row['lat']\n",
    "    lat1 = radians(lat1)\n",
    "    lat2 = radians(lat2)\n",
    "    delta_lon = radians(lon2 - lon1)\n",
    "    x = sin(delta_lon) * cos(lat2)\n",
    "    y = cos(lat1) * sin(lat2) - (sin(lat1) * cos(lat2) * cos(delta_lon))\n",
    "    initial_bearing = atan2(x, y)\n",
    "    initial_bearing = degrees(initial_bearing)\n",
    "    compass_bearing = (initial_bearing + 360) % 360\n",
    "    return compass_bearing\n",
    "\n",
    "def angular_difference(row):\n",
    "    diff = abs(row['prev_dir'] - row['dir'])\n",
    "    if diff > 180:\n",
    "        diff = abs(diff - 360)\n",
    "    return diff \n",
    "\n",
    "def compute_segment_info(df):\n",
    "    df = df.copy()\n",
    "    df['t'] = df.index\n",
    "    df['prev_t'] = df.groupby('id')['t'].shift()\n",
    "    df['diff_t_s'] = df.apply(time_difference, axis=1)\n",
    "    df['prev_lon'] = df.groupby('id')['lon'].shift()\n",
    "    df['prev_lat'] = df.groupby('id')['lat'].shift()\n",
    "    df['prev_x'] = df.groupby('id')['x'].shift()\n",
    "    df['prev_y'] = df.groupby('id')['y'].shift()\n",
    "    df['diff_x'] = df['x'] - df['prev_x']\n",
    "    df['diff_y'] = df['y'] - df['prev_y']\n",
    "    df['next_lon'] = df.groupby('id')['lon'].shift(-1)\n",
    "    df['next_lat'] = df.groupby('id')['lat'].shift(-1)\n",
    "    df['dist_prev_m'] = df.apply(distance_to_prev, axis=1)\n",
    "    df['dist_next_m'] = df.apply(distance_to_next, axis=1)\n",
    "    df['speed_m/s'] = df['dist_prev_m']/df['diff_t_s']\n",
    "    df['prev_speed'] = df.groupby('id')['speed_m/s'].shift()\n",
    "    df['diff_speed'] = df.apply(speed_difference, axis=1)\n",
    "    df['acceleration'] = df.apply(acceleration, axis=1)\n",
    "    df['dir'] = df.apply(direction, axis=1)\n",
    "    df['prev_dir'] = df.groupby('id')['dir'].shift()\n",
    "    df['diff_dir'] = df.apply(angular_difference, axis=1)\n",
    "    df = df.drop(columns=['prev_x', 'prev_y', 'next_lon', 'next_lat', 'prev_speed', 'prev_dir'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "try:\n",
    "    segment_df = pd.read_pickle('./segments.pkl')\n",
    "except:\n",
    "    segment_df = compute_segment_info(cropped_df)\n",
    "    segment_df.to_pickle(\"./segments.pkl\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "compute_segment_info(cropped_df[cropped_df.id==304752000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "easteregg = cropped_df[(cropped_df.id==636092484) | (cropped_df.id==636092478)]\n",
    "easteregg['id'] = 1\n",
    "segment_df = segment_df.append(compute_segment_info(easteregg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A-5) Gaps in trajectories\n",
    "\n",
    "Depending on the method used for splitting tracks into trajectories, the resulting trajectories can include gaps. These gaps can be due to technical failure of the tracking device, the mover leaving the observable area, deliberate deactivation of the tracking device, or (pre)processing issues. \n",
    "\n",
    "TODO: consequences\n",
    "\n",
    "TODO: method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAP_MIN = 10000\n",
    "GAP_MAX = 100000\n",
    "\n",
    "segment_df['is_gap'] = ( (segment_df['dist_prev_m']>GAP_MIN) & (segment_df['dist_prev_m']<GAP_MAX) ) | ( (segment_df['dist_next_m']>GAP_MIN) & (segment_df['dist_next_m']<GAP_MAX) ) \n",
    "segment_df['id_by_gap'] = segment_df.groupby(\"id\")['is_gap'].transform(lambda x: x.ne(x.shift()).cumsum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = [df[['x','y']] for name, df in segment_df[segment_df.is_gap].groupby(['id', 'id_by_gap']) ]\n",
    "path = hv.Path(grouped, kdims=['x','y'])\n",
    "plot = datashade(path, cmap=COLOR_HIGHLIGHT).opts(frame_height=FIGSIZE[1], frame_width=FIGSIZE[0])\n",
    "BG_TILES * plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B) Precision problems\n",
    "\n",
    "Precision issues in movement data may affect both spatial coordinates as well as timestamps of records. \n",
    "\n",
    "The following protocol steps target issues of excessively truncated coordinates and timestamps. \n",
    "\n",
    "\n",
    "### B-1) Coordinate imprecision\n",
    "\n",
    "This step addresses the question if the coordinates have been truncated excessively. Due to the limited accuracy of conventional GPS, one may argue that here is little benefit to more than five decimal places if coordinates are reported as latitude and longitude. However, rounding or truncating coordinates can lead to stair-shaped trajectories, particularly in densely sampled datasets. \n",
    "\n",
    "TODO: consequences\n",
    "\n",
    "Direction histograms are useful in revealing excessively truncated coordinates which results in an over-representation of direction values at 45 degree steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_df['dir'][segment_df.dist_prev_m>0].hvplot.hist(bins=72, title='Histogram of directions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B-2) Timestamp imprecision \n",
    "\n",
    "#### Truncated timestamps\n",
    "\n",
    "This step addresses the question if timestamps have been truncated excessively. Imprecise timestamps are the result of undue truncation or rounding in the date collection or (pre)processing workflow. Truncation can result in multiple position records of the same mover referring to the same time (Andrienko et al. 2016). Consecutive records with identical timestamps but different positions will result in zero-length time deltas between affected records and thus to division-by-zero errors when computing speeds. If positions are sparsely sampled, moderate truncation (for example, of milliseconds) will not result in multiple records with identical timestamps. \n",
    "\n",
    "TODO: consequences\n",
    "\n",
    "Counts of records per timestamp and mover ID can help identify cases of excessively truncated timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_zero_movement = segment_df[segment_df.dist_prev_m>0]\n",
    "\n",
    "n_per_id_t = non_zero_movement[['id', 't', 'x']].groupby(['id', 't']).count().reset_index()\n",
    "n_per_id_t['x'].plot.hist(title='Counts of records per timestamp and mover ID', log=True)\n",
    "#n_per_id_t.groupby('x').count().hvplot(title='Counts of records per timestamp and mover ID', y='id', logy=True)  # line plot not ideal\n",
    "#n_per_id_t['x'].hvplot.hist(title='Counts of records per timestamp and mover ID', logy=True)  # upstream bug in log scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates_per_id = n_per_id_t[n_per_id_t.x>1].drop(columns=['t']).groupby(['id']).count().rename(columns={'x':'n'})\n",
    "duplicates_per_id['n'].plot.hist(title='Count of duplicate timestamps per mover ID', log=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "duplicates_per_id.sort_values(by='n', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "plot_single_mover(segment_df, 230666000, date(2018,1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C) Consistency problems\n",
    "\n",
    "Datasets may not be as consistent with regards to collection parameters and covered movers as analysts expect. These problems usually cannot be detected from elementary position records. Therefore, intermediate segments or overall trajectories are needed. \n",
    "\n",
    "The following protocol steps target issues of heterogeneous sampling intervals and unexpected heterogeneous mover types. \n",
    "\n",
    "\n",
    "### C-1) Sampling heterogeneity\n",
    "\n",
    "#### Heterogeneous sampling intervals\n",
    "\n",
    "This step addresses the question whether the sampling frequency is stable. Some tracking systems provide records at regular time intervals. Other systems have rule-based sampling strategies. For example, in the Automatic Identification System (AIS), updates are more frequent when objects move quickly than when they stand still. Some GPS trackers may skip positions during straight-line movement (Andrienko et al. 2016). Other systems work on a best-effort base with a target sampling interval that may be exceeded if the system is busy.\n",
    "\n",
    "TODO: consequences\n",
    "\n",
    "Histograms of sampling intervals help determine whether sampling intervals are stable and, if yes, what the typical sampling interval is. If not, they show the range of observed sampling intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_df.diff_t_s.hvplot.hist(title='Histogram of intervals between consecutive records (in seconds)', bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_df[segment_df.diff_t_s<=120].diff_t_s.hvplot.hist(title='Histogram of intervals between consecutive records (in seconds)', bins=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_df.hvplot.scatter(title='Coordinate change plot', x='diff_x', y='diff_y', datashade=True, \n",
    "                          xlim=(-1000,1000), ylim=(-1000,1000), frame_width=FIGSIZE[1], frame_height=FIGSIZE[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C-2) Mover heterogeneity\n",
    "\n",
    "#### Heterogeneous mover types\n",
    "\n",
    "This step addresses the question whether the dataset contains heterogeneous types of movers. Datasets of human movement are expected to contain a mix of different transport modes. Other datasets, such as floating car data (FCD), are expected to be more heterogeneous, for example, to only contain car movements. However, errors in the collection process can invalidate this assumption. For example, if mobile (as opposed to built-in) trackers are used, they may be removed from vehicles and carried around by other means of transport. Other sources of heterogeneity are not due to errors but may still surprise the analysts. For example, AIS datasets also contain track from search and rescue vessels which include helicopters.\n",
    "\n",
    "TODO: consequences\n",
    "\n",
    "Scatterplots of different combinations of trajectory characteristics, such as total length, median speed, median direction change, and typical acceleration can help gain a better understanding of how heterogeneous the movers in a dataset are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_zero_speed = segment_df[(segment_df['speed_m/s']>0.1)]\n",
    "daily = non_zero_speed.groupby(['id', pd.Grouper(freq='d')]).agg({'dist_prev_m':'sum', 'speed_m/s':'median'}) \n",
    "\n",
    "daily.hvplot.scatter(title='Daily travelled distance over median speed (m/s)', x='dist_prev_m', y='speed_m/s', \n",
    "                    hover_cols=['id','time'], frame_width=FIGSIZE[1], frame_height=FIGSIZE[1], alpha=0.3, \n",
    "                    xlim=(-100000,1500000), ylim=(-10,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_paths(original_df, title='', add_bg=True):\n",
    "    grouped = [df[['x','y']] for name, df in original_df.groupby(['id']) ]\n",
    "    path = hv.Path(grouped, kdims=['x','y'])\n",
    "    plot = datashade(path, cmap=COLOR_HIGHLIGHT).opts(title=title, frame_height=FIGSIZE[1], frame_width=FIGSIZE[0])\n",
    "    if add_bg:\n",
    "        return BG_TILES * plot\n",
    "    else: \n",
    "        return plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speedsters = daily[daily['speed_m/s']>20].reset_index().id.unique()\n",
    "speedsters = segment_df[segment_df.id.isin(speedsters)]\n",
    "plot_paths(speedsters, title='Speedsters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longdist = daily[daily['dist_prev_m']>800000].reset_index().id.unique()\n",
    "longdist = segment_df[segment_df.id.isin(longdist)]\n",
    "plot_paths(longdist, title='Long distance travelers')\n",
    "\n",
    "#grouped = [df[['x','y']] for name, df in longdist.groupby(['id']) ]\n",
    "#path = hv.Path(grouped, kdims=['x','y'])\n",
    "#plot = datashade(path, cmap=COLOR_HIGHLIGHT).opts(title='Long distance travelers', frame_height=FIGSIZE[1], frame_width=FIGSIZE[0])\n",
    "#BG_TILES * plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA PREPARATION: Computing trajectory information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MINIMUM_NUMBER_OF_RECORDS = 100\n",
    "MINIMUM_SPEED_MS = 1\n",
    "\n",
    "def reset_values_at_daybreaks(tmp, columns):\n",
    "    tmp['ix'] = tmp.index\n",
    "    tmp['zero'] = 0\n",
    "    ix_first = tmp.groupby(['id', pd.Grouper(freq='d')]).first()['ix']\n",
    "    for col in columns:\n",
    "        tmp[col] = tmp['zero'].where(tmp['ix'].isin(ix_first), tmp[col])\n",
    "    tmp = tmp.drop(columns=['zero', 'ix'])\n",
    "    return tmp\n",
    "\n",
    "tmp = segment_df.copy()\n",
    "tmp['acceleration_abs'] = np.abs(tmp['acceleration'])\n",
    "tmp['diff_speed_abs'] = np.abs(tmp['diff_speed'])\n",
    "tmp = tmp.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "tmp = reset_values_at_daybreaks(tmp, ['diff_t_s','dist_prev_m','diff_speed_abs','acceleration_abs'])\n",
    "\n",
    "traj_df = tmp.groupby(['id', pd.Grouper(freq='d')]) \\\n",
    "    .agg({'diff_t_s':['median', 'sum'], \n",
    "          'speed_m/s':['median','std'],\n",
    "          'diff_dir':['median','std'], \n",
    "          'dist_prev_m':['median', 'sum'], \n",
    "          'diff_speed_abs':['max'], \n",
    "          'acceleration_abs':['median','max','mean','std'], \n",
    "          't':['min','count'],\n",
    "         'shiptype':lambda x:x.value_counts().index[0]}) \n",
    "\n",
    "traj_df.columns = [\"_\".join(x) for x in traj_df.columns.ravel()]\n",
    "traj_df = traj_df.rename(columns={'t_count':'n', 'shiptype_<lambda>':'shiptype', \n",
    "                                  'diff_t_s_sum':'duration_s', 'dist_prev_m_sum':'length_m'})\n",
    "traj_df['length_km'] = traj_df['length_m'] / 1000\n",
    "traj_df['duration_h'] = traj_df['duration_s'] / 3600\n",
    "traj_df['t_min_h'] = traj_df['t_min'].dt.hour + traj_df['t_min'].dt.minute / 60\n",
    "\n",
    "traj_df = traj_df[traj_df.n>=MINIMUM_NUMBER_OF_RECORDS]\n",
    "traj_df = traj_df[traj_df['speed_m/s_median']>=MINIMUM_SPEED_MS]\n",
    "traj_df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plot_single_mover(segment_df, 636092484, date(2017,7,1)) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plot_single_mover(segment_df, 636092478, date(2017,7,1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvplot.scatter_matrix(\n",
    "    traj_df[['length_km', 'speed_m/s_median', 'duration_h', 'acceleration_abs_mean', 'diff_dir_median']]\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "tmp = traj_df.reset_index().copy()\n",
    "tmp = tmp[['length_km', 'duration_h', 'diff_t_s_median', 'dist_prev_m_median', 'speed_m/s_median', \n",
    "            'acceleration_abs_median', 'diff_dir_median', 'shiptype']]\n",
    "\n",
    "hvplot.parallel_coordinates(\n",
    "    tmp[:100], 'shiptype', title='Trajectory properties', logy=True,\n",
    "    frame_width=FIGSIZE[0], frame_height=FIGSIZE[1]+20\n",
    ").opts(xrotation=90, legend_position='right')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "tmp = traj_df.reset_index().copy()\n",
    "tmp = tmp[['duration_h','diff_t_s_median', 'length_km', 'dist_prev_m_median', 'speed_m/s_median', \n",
    "            'acceleration_abs_median', 'diff_dir_median', 'shiptype']]\n",
    "\n",
    "hvplot.parallel_coordinates(\n",
    "    tmp[tmp.shiptype.isin(['SAR'])], 'shiptype', title='Trajectory properties', logy=True,\n",
    "    frame_width=FIGSIZE[0], frame_height=FIGSIZE[1]+20\n",
    ").opts(xrotation=90, legend_position='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C-3) Tracker heterogeneity\n",
    "\n",
    "#### Heterogeneous trackers\n",
    "\n",
    "This step addresses the question whether the dataset contains records from devices with different tracking characteristics. Devices with GPS tracking capabilities vary widely in performance. For example, when data is collected using smartphone apps, coordinates may have passed through a variety of (not always fully transparent) preprocessing steps that depend on the operating system version and hardware manufacturer (citation needed).  \n",
    "\n",
    "TODO: consequences\n",
    "\n",
    "Effects of heterogeneous trackers can be hard to distinguish from effects of heterogeneous movers. Tracker heterogeneity may result in sampling rates and/or spatial accuracy that differ between movers. Furthermore, differences in the availability of additional attribute data within movement records may point towards tracker heterogeneity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "traj_df[(traj_df['diff_t_s_median']<=120) & (traj_df['speed_m/s_median']>0)] \\\n",
    "    .hvplot.scatter(\n",
    "        title='Median sampling interval over median speed', alpha=0.3,\n",
    "        x='diff_t_s_median', y='speed_m/s_median', hover_cols=['id','time'], #datashade=True,\n",
    "        frame_width=FIGSIZE[1], frame_height=FIGSIZE[1], ylim=(-10,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D) Accuracy problems \n",
    "\n",
    "Incorrect mover identities, coordinates, and timestamps can affect movement data analyses in a variety of ways. These problems usually cannot be detected from elementary position records. Therefore, intermediate segments or overall trajectories are needed. \n",
    "\n",
    "The following protocol steps target issues of mover identity, as well as spatial and temporal inaccuracy. \n",
    "\n",
    "\n",
    "### D-1) Mover identity issues \n",
    "\n",
    "Reliable mover identifiers are needed to identify which movement data records belong to the same mover. Identity issues occur when ids are not unique, i.e. if multiple movers are assigned the same identifier. A single mover may also be referred to by multiple different identifiers, either at the same time or due to changes over time. This can happen because the data collection system or (pre)processing workflow reassigns identifiers based on business rules or in regular time intervals. \n",
    "\n",
    "#### Non-unique IDs\n",
    "\n",
    "This step addresses the question whether the dataset contains cases of non-unique identifiers. Due to misconfiguration of trackers or (pre)processing errors, the same identifier may be assigned to multiple movers simultaneously (or to different movers over time which is covered by the Unstable IDs step). Simultaneous non-unique IDs result in trajectories that connect location records by multiple movers traveling on their distinct paths. The resulting trajectory therefore jumps between locations along these different paths. Consequently, the trajectory assumes a zig-zag shape and speeds derived from consecutive location records assume unrealistic values. \n",
    "\n",
    "TODO: consequences\n",
    "\n",
    "Scatterplots of trajectory length and direction change are useful to identify cases of non-unique IDs.\n",
    "Since movers with idetnical IDs rarely travel in close vicinity, potential candidates for non-unique IDs are characterized by long trajectories with high direction change values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_df.hvplot.scatter(\n",
    "    title='Trajectory length over direction difference (median)', alpha=0.3,\n",
    "    x='length_km', y='diff_dir_median', hover_cols=['id','time'], #datashade=True,\n",
    "    frame_width=FIGSIZE[1], frame_height=250#, ylim=(-10,100)\n",
    ") + traj_df.sort_values(by='length_km', ascending=False)[:10][['length_km', 'speed_m/s_median', 'diff_dir_median']].hvplot.table(\n",
    "    title='Top 10 trajectories - length', frame_width=FIGSIZE[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_single_mover(segment_df, 1, date(2017,7,1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unstable IDs\n",
    "\n",
    "This step addresses the question whether mover identifiers are stable and for how long they remain stable. Some data sources do not provide permanently stable identifiers. Systems may reassign identifiers based on business rules or in regular time intervals. For example, taxi floating car systems may not include stable vehicle IDs, instead relying on trip IDs that are reassigned whenever a taxi finishes a trip. \n",
    "\n",
    "TODO: consequences\n",
    "\n",
    "Scatterplots of trajectory duration versus start time are useful to find out how often IDs change and whether they tend to change at the same time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvplot.scatter_matrix(traj_df[['t_min_h', 'duration_h']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D-2) Spatial inaccuracy \n",
    "\n",
    "Coordinate errors range from basic noise due to the inherent inaccuracy of GPS to unrealistic jumps caused by technical errors or deliberate action. \n",
    "\n",
    "#### Outliers with unrealistic jumps\n",
    "\n",
    "This step addresses the question if trajectories contain unrealistic jumps that require data cleaning. These jumps result in unrealistic derived speed values. The limit for being unrealistic depends on the use case. For example, for ground-based transport, Fillekes et al. (2019) set the limit at 330 km/h based on the maximum speed of German high-speed trains. \n",
    "\n",
    "TODO: consequences\n",
    "\n",
    "Histograms of derived speed between consecutive location records are useful to see if there is a long tail of high speed values.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_df['speed_m/s'].hvplot.hist(\n",
    "    title='Histogram of speed between consecutive records', bins=100, frame_width=FIGSIZE[1], frame_height=250\n",
    ") + segment_df.sort_values(by='speed_m/s', ascending=False)[:10][['id', 'speed_m/s']].hvplot.table(\n",
    "    title='Top 10 records - speed', frame_width=FIGSIZE[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_single_mover(segment_df, 218057000, date(2018,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_single_mover(segment_df, 219348000, date(2017,7,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jitter / noise\n",
    "\n",
    "This step addresses the question of how noisy the trajectories are. GPS noise causes a systematic \"overestimation of distance\" when the sampling frequency is high (Ranacher et al. 2016 IJGIS). On the other hand, distances are underestimated when the sampling frequency is low. Without evaluating the sampling frequency, distance and derived speed values therefore are insufficient to understand noise. Noise also affects trajectories of movers that are standing still, appearing as fake jittery movement. \n",
    "\n",
    "TODO: consequences\n",
    "\n",
    "Trajectory plots are an intuitive way to evaluate small sets of trajectories. However, in some cases, trajectories that appear noisy can reflect real movement patterns. For example, vessel routes may have a zig-zag shape in case of adverse weather conditions (Patroumpas et al. 2017). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_df.hvplot.scatter(\n",
    "    title='Direction difference median over standard deviation', alpha=0.3,\n",
    "    x='diff_dir_median', y='diff_dir_std', hover_cols=['id','time'], #datashade=True,\n",
    "    frame_width=FIGSIZE[1], frame_height=250, ylim=(-10,100)\n",
    ") + traj_df.sort_values(by='diff_dir_median', ascending=False)[:10][['diff_dir_median','diff_dir_std']].hvplot.table(\n",
    "    title='Top 10 trajectories - direction difference', frame_width=FIGSIZE[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_single_mover(segment_df, 244063000, date(2018,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_single_mover(segment_df, 220614000, date(2018,1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D-3) Temporal inaccuracy\n",
    "\n",
    "Timestamp errors potentially affect the synchronization between trajectories as well as the order of records within individual trajectories. \n",
    "\n",
    "#### Time zone and daylight saving issues\n",
    "\n",
    "This step addresses the question how time zones and daylight saving affect the dataset. In some datasets, time zone information may be included with each time stamp. However usually, this is not the case and analysts have to resort to metadata or documentation which are not always comprehensive or reliable. Time zone issues can be hard to detect, particularly if the dataset contains tracks from multiple time zones but the zone information got lost along the way. This issues may be discovered due to unexpected derived movement patterns, such as, for example, significant numbers of people leaving their homes in the middle of the night or excessive movement of nocturnal animals during the day. \n",
    "\n",
    "TODO: consequences\n",
    "\n",
    "Temporal histograms of record counts are helpful to detect gaps or double counting when daylight saving goes into and out of effect. Temporal heatmaps of movement properties, such as speed, can help recognize time zone issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = segment_df[segment_df['speed_m/s']>1]\n",
    "hourly = tmp['id'].groupby([tmp.index.hour, pd.Grouper(freq='d')]).count().to_frame(name='n')\n",
    "hourly.rename_axis(['hour', 'day'], inplace=True)\n",
    "hourly.hvplot.heatmap(title='Count of records with speed > 1m/s', x='hour', y='day', C='n', width=FIGSIZE[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Out-of-sequence positions\n",
    "\n",
    "This step addresses the question if records belonging to a trajectory appear out of sequence. A closely related problem is when a mover appears at two different locations at the same time. These problems can happen in systems that do not provide tracker timestamps and instead use receiver or storage time. For example, the Automatic Identification System (AIS) protocol does not transmit tracker timestamps and instead provides only offsets (in second) from the previously transmitted message which is insufficient to establish temporal order \"since positional updates from a single vessel may come from a series of base stations (those within range of its antenna along the route).\" (Patroumpas et al. 2017) \n",
    "\n",
    "TODO: consequences\n",
    "\n",
    "Scatterplots of acceleration versus direction change are helpful to distinguish out-of-sequence problems from large jumps caused by location errors since out-of-sequence records results in sudden reversals of the movement direction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_df.hvplot.scatter(\n",
    "    title='Direction difference (median) over speed (median)', alpha=0.3,\n",
    "    x='diff_dir_median', y='acceleration_abs_median', hover_cols=['id','time'], #datashade=True,\n",
    "    frame_width=FIGSIZE[1], frame_height=250#, ylim=(-10,100)\n",
    ") + traj_df.sort_values(by='diff_dir_median', ascending=False)[:10][['diff_dir_median','diff_dir_std']].hvplot.table(\n",
    "    title='Top 10 trajectories - direction difference', frame_width=FIGSIZE[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_single_mover(segment_df, 308322000, date(2017,7,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_single_mover(segment_df, 265615040, date(2017,7,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix -- Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_distance(row):\n",
    "    lon1 = row['prev_lon']\n",
    "    lat1 = row['prev_lat']\n",
    "    lon2 = row['lon']\n",
    "    lat2 = row['lat']\n",
    "    delta_lat = radians(lat2 - lat1)\n",
    "    delta_lon = radians(lon2 - lon1)\n",
    "    a = sin(delta_lat/2) * sin(delta_lat/2) + cos(radians(lat1)) * cos(radians(lat2)) * sin(delta_lon/2) * sin(delta_lon/2)\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    dist = R_EARTH * c\n",
    "    return dist\n",
    "\n",
    "def connect_pts(row):\n",
    "    lon1 = row['prev_lon']\n",
    "    lat1 = row['prev_lat']\n",
    "    lon2 = row['lon']\n",
    "    lat2 = row['lat']\n",
    "    return LineString([(lon1, lat1), (lon2, lat2)])\n",
    "\n",
    "def find_gaps(df, min_dist, max_dist):\n",
    "    if len(df)<2:\n",
    "        return None\n",
    "    i = df.copy()\n",
    "    i = i.assign(prev_lon=i.lon.shift())\n",
    "    i = i.assign(prev_lat=i.lat.shift())\n",
    "    i = i.assign(dist=i.apply(compute_distance, axis=1))\n",
    "    i = i[(i.dist>min_dist) & (i.dist<max_dist)]\n",
    "    if len(i)==0: \n",
    "        return None\n",
    "    i = i.assign(geometry=i.apply(connect_pts, axis=1))\n",
    "    return i\n",
    "\n",
    "def make_gap_gdf(df, min_dist, max_dist):\n",
    "    a = None\n",
    "    for the_id in df.id.unique():\n",
    "        i = df[df.id==the_id]\n",
    "        gaps_df = find_gaps(i, min_dist, max_dist)\n",
    "        if gaps_df is None:\n",
    "            continue\n",
    "        if a is None: \n",
    "            a = gaps_df\n",
    "        else:\n",
    "            a = a.append(gaps_df)\n",
    "    if a is not None:\n",
    "        return gpd.GeoDataFrame(a, geometry='geometry')\n",
    "\n",
    "def plot_gaps(df, min_dist, max_dist, width=FIGSIZE[0], height=FIGSIZE[1]):\n",
    "    gaps_gdf = make_gap_gdf(df, min_dist, max_dist)\n",
    "    if gaps_gdf is not None:\n",
    "        plot = gaps_gdf.hvplot(geo=True, color=COLOR_HIGHLIGHT, frame_width=width, frame_height=height)\n",
    "        return tiles.OSM() * plot   "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plot_gaps(cropped_df, min_dist=10000, max_dist=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = df[(df['id']==304752000) | (df['id']==257024000)]\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in sample_df.groupby('id'):\n",
    "    print(f'{name}: {len(df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "grouped = [df[['id','x','y']] for name, df in sample_df.groupby(['id', pd.Grouper(freq='d')])]\n",
    "path = hv.Path(grouped, kdims=['x','y'], vdims=['id','x']).opts(line_width=2, width=600, color=COLOR)\n",
    "plot = datashade(path).opts(frame_height=FIGSIZE[1], frame_width=FIGSIZE[0])\n",
    "BG_TILES * plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = [df[['x','y']] for name, df in cropped_df.groupby(['id', pd.Grouper(freq='d')]) if len(df)>100]\n",
    "path = hv.Path(grouped, kdims=['x','y'])\n",
    "plot = datashade(path).opts(frame_height=FIGSIZE[1], frame_width=FIGSIZE[0])\n",
    "BG_TILES * plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = cropped_df\n",
    "\n",
    "a = None\n",
    "for the_id in tmp.id.unique():\n",
    "    i = tmp[tmp.id==the_id].copy()\n",
    "    i = i.assign(prev_lon=i.lon.shift())\n",
    "    i = i.assign(prev_lat=i.lat.shift())\n",
    "    i = i.assign(dist=i.apply(compute_distance, axis=1))    \n",
    "    #i = find_gaps(i, 1000, 10000000)\n",
    "    plot = hv.Path(i, kdims=['x','y'], vdims=['id', 'dist']).opts(color='dist', line_width=4)\n",
    "    plot = datashade(plot, normalization='linear', aggregator=ds.by('id', ds.min(\"dist\")))\n",
    "    #plot = datashade(plot, normalization='linear')\n",
    "    if a is None: a = plot\n",
    "    else: a = a * plot\n",
    "tiles.OSM() * a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a * sample_df.hvplot.scatter(x='x', y='y', datashade=True, by='id', frame_width=FIGSIZE[0], frame_height=FIGSIZE[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datashade(hv.Path(sample_df, kdims=['x','y']), normalization='linear', aggregator=ds.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = sample_df[['id','x','y']]\n",
    "hv.Path(tmp[tmp.id==304752000], kdims=['x','y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xxx = compute_segment_info(cropped_df)\n",
    "xxx[xxx.is_gap]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_values_at_daybreaks(tmp, columns):\n",
    "    tmp['ix'] = tmp.index\n",
    "    tmp['zero'] = 0\n",
    "    ix_first = tmp.groupby(['id', pd.Grouper(freq='d')]).first()['ix']\n",
    "    for col in columns:\n",
    "        tmp[col] = tmp['zero'].where(tmp['ix'].isin(ix_first), tmp[col])\n",
    "    tmp = tmp.drop(columns=['zero', 'ix'])\n",
    "    return tmp\n",
    "\n",
    "tmp = segment_df[segment_df.id==5322].copy()\n",
    "tmp['acceleration_abs'] = np.abs(tmp['acceleration'])\n",
    "tmp['diff_speed_abs'] = np.abs(tmp['diff_speed'])\n",
    "\n",
    "tmp = reset_values_at_daybreaks(tmp, ['diff_t_s','dist_prev_m','diff_speed_abs','acceleration_abs'])\n",
    "\n",
    "traj_df = tmp.groupby(['id', pd.Grouper(freq='d')]) \\\n",
    "    .agg({'diff_t_s':['median', 'sum'], \n",
    "          'speed_m/s':['median','var'],\n",
    "          'diff_dir':['median'], \n",
    "          'dist_prev_m':['median', 'sum'], \n",
    "          'diff_speed_abs':['max'], \n",
    "          'acceleration_abs':['median','max','mean','var'], \n",
    "          't':['min','count'],\n",
    "         'shiptype':lambda x:x.value_counts().index[0]}) \n",
    "\n",
    "traj_df.columns = [\"_\".join(x) for x in traj_df.columns.ravel()]\n",
    "traj_df = traj_df.rename(columns={'t_count':'n', 'shiptype_<lambda>':'shiptype', \n",
    "                                  'diff_t_s_sum':'duration_s', 'dist_prev_m_sum':'length_m'})\n",
    "traj_df['length_km'] = traj_df['length_m'] / 1000\n",
    "traj_df['duration_h'] = traj_df['duration_s'] / 3600\n",
    "traj_df['t_min_h'] = traj_df['t_min'].dt.hour + traj_df['t_min'].dt.minute / 60\n",
    "\n",
    "traj_df = traj_df[traj_df.n>=100]\n",
    "traj_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix_first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
